plt.plot(x, quad_preds[i], "g-")
plt.plot(3, 3**2, "o")
plt.legend(["Linear predictions", "Quadratic predictions"])
plt.show()
lin_preds[0].shape
lin_preds[0][x==3]
lin_preds[0]
np.where(x == 3)
x
x
np.where(x == 3)
np.where(x == 3)[0]
lin_preds[0][np.where(x == 3)[0]
lin_preds[0][np.where(x == 3)[0]]
lin_preds[0][np.where(x == 3)[0]]
bias_lin = np.mean([i[np.where(x == 3)[0]] for i in lin_preds]) - 3**2
bias_quad = np.mean([i[np.where(x == 3)[0]] for i in quad_preds]) - 3**2
bias_quint = np.mean([i[np.where(x == 3)[0]] for i in quint_preds]) - 3**2
var_lin = np.var([i[x == 3] for i in lin_preds])
var_quad = np.var([i[x == 3] for i in quad_preds])
bias_lin = np.mean([i[x_highlight] for i in lin_preds]) - 3**2
bias_quad = np.mean([i[x_highlight] for i in quad_preds]) - 3**2
bias_quint = np.mean([i[x_highlight] for i in quint_preds]) - 3**2
x_highlight = np.where(x == 3)[0]
bias_lin = np.mean([i[x_highlight] for i in lin_preds]) - 3**2
bias_quad = np.mean([i[x_highlight] for i in quad_preds]) - 3**2
bias_quint = np.mean([i[x_highlight] for i in quint_preds]) - 3**2
var_lin = np.var([i[x_highlight] for i in lin_preds])
var_quad = np.var([i[x_highlight] for i in quad_preds])
var_quint = np.var([i[x_highlight] for i in quint_preds])
bias_lin
bias_quad
bias_quint
var_lin
var_quad
var_quint
# 2.3.iii
x_highlight = np.where(x == 3)[0]
bias_lin = np.mean([i[x_highlight] for i in lin_preds]) - 3**2
bias_quad = np.mean([i[x_highlight] for i in quad_preds]) - 3**2
bias_quint = np.mean([i[x_highlight] for i in quint_preds]) - 3**2
var_lin = np.var([i[x_highlight] for i in lin_preds])
var_quad = np.var([i[x_highlight] for i in quad_preds])
var_quint = np.var([i[x_highlight] for i in quint_preds])
bias_lin
bias_quad
bias_quint
var_lin
var_quad
var_quint
# Inspect
bias_lin
bias_quad
bias_quint
var_lin
var_quad
var_quint
epsilon = np.random.normal(scale=5, size=100)
MSE_lin = bias_lin**2 + var_lin + np.var(epsilon)
MSE_quad = bias_quad**2 + var_quad + np.var(epsilon)
MSE_quint = bias_quint**2 + var_quint + np.var(epsilon)
MSE_lin
MSE_quad
MSE_quint
# 2.3.v
MSE_lin = bias_lin**2 + var_lin + np.var(epsilon)
MSE_quad = bias_quad**2 + var_quad + np.var(epsilon)
MSE_quint = bias_quint**2 + var_quint + np.var(epsilon)
MSE_lin
MSE_quad
MSE_quint
np.random.seed(7)
# 2.3
samples = [(np.random.normal(scale=5, size=len(x)) + square(x[:,0])) for i in range(100)]
len(samples) # Inspection of samples
len(samples[0])
samples[0]
# 2.3.i
quadratic = PolynomialFeatures(degree=2)  # Unsure what goes on here, ask Lau
quintic = PolynomialFeatures(degree=5)  # Unsure what goes on here, ask Lau
X_quadratic = quadratic.fit_transform(x.reshape(-1, 1))
X_quintic = quintic.fit_transform(x.reshape(-1, 1))
lin_preds = []
quad_preds = []
quint_preds = []
for i in samples:
# Linear
lin_regressor = LinearRegression(fit_intercept=False)
lin_regressor.fit(x, i)
lin_preds.append(lin_regressor.predict(x))
# Quadratic
quad_regressor = LinearRegression(fit_intercept=False)
quad_regressor.fit(X_quadratic, i)
quad_preds.append(quad_regressor.predict(X_quadratic))
# Quintic
quint_regressor = LinearRegression(fit_intercept=False)
quint_regressor.fit(X_quintic, i)
quint_preds.append(quint_regressor.predict(X_quintic))
# Plotting linear and quadratic fit
plt.figure()
for i in range(0, len(quad_fit_predictions)):
plt.plot(x, lin_preds[i], "y-")
plt.plot(x, quad_preds[i], "g-")
plt.plot(3, 3**2, "o")
plt.legend(["Linear predictions", "Quadratic predictions"])
plt.show()
# 2.3.ii
plt.figure()
for i in range(0, len(quad_fit_predictions)):
plt.plot(x, quad_preds[i], "g-")
plt.plot(x, quint_preds[i], "b-")
plt.plot(3, 3**2, "o")
plt.legend(["Quadratic predictions", "Quintic predictions"])
plt.show()
# Which models have highest bias, and which have highest variance?
# Linear model highest bias
# Quintic model highest variance
# 2.3.iii
x_highlight = np.where(x == 3)[0]
bias_lin = np.mean([i[x_highlight] for i in lin_preds]) - 3**2
bias_quad = np.mean([i[x_highlight] for i in quad_preds]) - 3**2
bias_quint = np.mean([i[x_highlight] for i in quint_preds]) - 3**2
var_lin = np.var([i[x_highlight] for i in lin_preds])
var_quad = np.var([i[x_highlight] for i in quad_preds])
var_quint = np.var([i[x_highlight] for i in quint_preds])
# Inspect
bias_lin
bias_quad
bias_quint
var_lin
var_quad
var_quint
# 2.3.iv
epsilon = np.random.normal(scale=5, size=100)
MSE_lin = bias_lin**2 + var_lin + np.var(epsilon)
MSE_quad = bias_quad**2 + var_quad + np.var(epsilon)
MSE_quint = bias_quint**2 + var_quint + np.var(epsilon)
MSE_lin
MSE_quad
MSE_quint
np.random.seed(7)
# 2.3
samples = [(np.random.normal(scale=5, size=len(x)) + square(x[:,0])) for i in range(100)]
len(samples) # Inspection of samples
len(samples[0])
samples[0]
# 2.3.i
quadratic = PolynomialFeatures(degree=2)  # Unsure what goes on here, ask Lau
quintic = PolynomialFeatures(degree=5)  # Unsure what goes on here, ask Lau
X_quadratic = quadratic.fit_transform(x.reshape(-1, 1))
X_quintic = quintic.fit_transform(x.reshape(-1, 1))
lin_preds = []
quad_preds = []
quint_preds = []
for i in samples:
# Linear
lin_regressor = LinearRegression(fit_intercept=False)
lin_regressor.fit(x, i)
lin_preds.append(lin_regressor.predict(x))
# Quadratic
quad_regressor = LinearRegression(fit_intercept=False)
quad_regressor.fit(X_quadratic, i)
quad_preds.append(quad_regressor.predict(X_quadratic))
# Quintic
quint_regressor = LinearRegression(fit_intercept=False)
quint_regressor.fit(X_quintic, i)
quint_preds.append(quint_regressor.predict(X_quintic))
# Plotting linear and quadratic fit
plt.figure()
for i in range(0, len(quad_fit_predictions)):
plt.plot(x, lin_preds[i], "y-")
plt.plot(x, quad_preds[i], "g-")
plt.plot(3, 3**2, "o")
plt.legend(["Linear predictions", "Quadratic predictions"])
plt.show()
# 2.3.ii
plt.figure()
for i in range(0, len(quad_fit_predictions)):
plt.plot(x, quad_preds[i], "g-")
plt.plot(x, quint_preds[i], "b-")
plt.plot(3, 3**2, "o")
plt.legend(["Quadratic predictions", "Quintic predictions"])
plt.show()
# Which models have highest bias, and which have highest variance?
# Linear model highest bias
# Quintic model highest variance
# 2.3.iii
x_highlight = np.where(x == 3)[0]
bias_lin = np.mean([i[x_highlight] for i in lin_preds]) - 3**2
bias_quad = np.mean([i[x_highlight] for i in quad_preds]) - 3**2
bias_quint = np.mean([i[x_highlight] for i in quint_preds]) - 3**2
var_lin = np.var([i[x_highlight] for i in lin_preds])
var_quad = np.var([i[x_highlight] for i in quad_preds])
var_quint = np.var([i[x_highlight] for i in quint_preds])
# 2.3.iv
# Inspect
bias_lin
bias_quad
bias_quint
var_lin
var_quad
var_quint
# 2.3.v
epsilon = np.random.normal(scale=5, size=100)
MSE_lin = bias_lin**2 + var_lin + np.var(epsilon)
MSE_quad = bias_quad**2 + var_quad + np.var(epsilon)
MSE_quint = bias_quint**2 + var_quint + np.var(epsilon)
MSE_lin
MSE_quad
MSE_quint
quit
library(reticulate)
print(conda_list())
reticulate::repl_python()
## assignment is done and only done with "=" (no arrows)
a = 2
# a <- 2 # results in a syntax error
## already assigned variables can be reassigned with basic arithmetic operations
a += 2
print(a)
a -= 1
print(a)
a *= 4
print(a)
a //= 2 # integer division
print(a)
a /= 2 # float  (numeric from R) division
print(a)
a **= 3 # exponentiation
print(a)
## lists (mutable)
a_list = [1, 2] # initiate a list (the square brackets) with the integers 1 and 2
b = a_list ## b now points to a_list, not to a new list with the integers 1 and 2
a_list.append(3) # add a new value to the end of the list
print(a_list)
print(b) # make sure you understand this
print(a_list[0]) # zero-indexing
print(a_list[1])
new_list = [0, 1, 2, 3, 4, 5] # slicing
print(new_list[0:3])
for index in range(0, 5): # indentation (use tabulation) controls scope of control variables (no brackets necessary),
if index == 0: # remember the colon
value = 0
else:
value += index
print(value)
this_is_true = True # logical values
this_is_false = False
# define functions using def
def fix_my_p_value(is_it_supposed_to_be_significant):
if is_it_supposed_to_be_significant:
p = 0.01
else:
p = 0.35
return(p)
print(fix_my_p_value(True))
# importing packages (similar to library)
import numpy # methods of numpy can now be accessed as below
print(numpy.arange(1, 10)) # see the dot
print(numpy.abs(-3))
import numpy as np # you can import them with another name than its default
print(np.cos(np.pi))
from numpy import pi, arange # or you can import specific methods
print(arange(1, 7))
print(pi)
matrix = np.ones(shape=(5, 5)) # create a matrix of ones
identity = np.identity(5) # create an identity matrix (5x5)
identity[:, 2] = 5 # exchange everything in the second column with 5's
## no dots in names - dots indicate applying a method
import matplotlib.pyplot as plt
plt.figure() # create new figure
plt.plot([1, 2], [1, 2], 'b-') # plot a blue line
plt.show() # show figure
plt.plot([2, 1], [2, 1], 'ro') # scatter plot (red)
plt.show()
plt.xlabel('a label')
plt.title('a title')
plt.show()
# Chunk 1
library(reticulate)
print(conda_list())
# Chunk 2
## assignment is done and only done with "=" (no arrows)
a = 2
# a <- 2 # results in a syntax error
## already assigned variables can be reassigned with basic arithmetic operations
a += 2
print(a)
a -= 1
print(a)
a *= 4
print(a)
a //= 2 # integer division
print(a)
a /= 2 # float  (numeric from R) division
print(a)
a **= 3 # exponentiation
print(a)
## lists (mutable)
a_list = [1, 2] # initiate a list (the square brackets) with the integers 1 and 2
b = a_list ## b now points to a_list, not to a new list with the integers 1 and 2
a_list.append(3) # add a new value to the end of the list
print(a_list)
print(b) # make sure you understand this
print(a_list[0]) # zero-indexing
print(a_list[1])
new_list = [0, 1, 2, 3, 4, 5] # slicing
print(new_list[0:3])
for index in range(0, 5): # indentation (use tabulation) controls scope of control variables (no brackets necessary),
if index == 0: # remember the colon
value = 0
else:
value += index
print(value)
this_is_true = True # logical values
this_is_false = False
# define functions using def
def fix_my_p_value(is_it_supposed_to_be_significant):
if is_it_supposed_to_be_significant:
p = 0.01
else:
p = 0.35
return(p)
print(fix_my_p_value(True))
# importing packages (similar to library)
import numpy # methods of numpy can now be accessed as below
print(numpy.arange(1, 10)) # see the dot
print(numpy.abs(-3))
import numpy as np # you can import them with another name than its default
print(np.cos(np.pi))
from numpy import pi, arange # or you can import specific methods
print(arange(1, 7))
print(pi)
matrix = np.ones(shape=(5, 5)) # create a matrix of ones
identity = np.identity(5) # create an identity matrix (5x5)
identity[:, 2] = 5 # exchange everything in the second column with 5's
## no dots in names - dots indicate applying a method
import matplotlib.pyplot as plt
plt.figure() # create new figure
plt.plot([1, 2], [1, 2], 'b-') # plot a blue line
plt.show() # show figure
plt.plot([2, 1], [2, 1], 'ro') # scatter plot (red)
plt.show()
plt.xlabel('a label')
plt.title('a title')
plt.show()
# Chunk 3
# Exercise 1.1
import numpy as np
np.random.seed(7) # for reproducibility
x = np.arange(10)
y = 2 * x
y = y.astype(float)
n_samples = len(y)
y += np.random.normal(loc=0, scale=1, size=n_samples)
X = np.zeros(shape=(n_samples, 2))
X[:, 0] = x ** 0
X[:, 1] = x ** 1
# Chunk 4
import matplotlib.pyplot as plt
beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
print(beta_hat)
y_hat = beta_hat[0] + beta_hat[1] * x
print(y_hat)
plt.figure()
plt.plot(x, y, 'bo')
plt.plot(x, y_hat, 'r')
plt.show()
# Chunk 5
# Exercise 1.2
y1 = np.array([3, 2, 7, 6, 9])
y2 = np.array([10, 4, 2, 1, -3])
y3 = np.array([15, -2, 0, 0, 3])
y = np.concatenate((y1, y2, y3))
# Chunk 6
X = np.zeros(shape=(len(y), 3))
X[0:5,   0] = 1
X[5:10,  1] = 1
X[10:15, 2] = 1
beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
print(beta_hat)
X2 = np.zeros(shape=(len(y), 3))
X2[0:5,   0] = 1
X2[:,     1] = 1
X2[10:15, 2] = 1
beta_hat2 = np.linalg.inv(X2.T @ X2) @ X2.T @ y
print(beta_hat2)
# Chunk 7
from scipy.stats import f
X3 = np.ones(shape=len(y))
beta_hat3 = 1 / (X3.T @ X3) * X3.T @ y
len_beta_hat3 = 1
y_hat2 = X2 @ beta_hat2
y_hat3 = X3 * beta_hat3
RSS2 = np.sum((y - y_hat2)**2)
RSS3 = np.sum((y - y_hat3)**2)
F = ((RSS3 - RSS2) / (len(beta_hat2) - len_beta_hat3)) / \
((RSS2) / (len(y) - len(beta_hat2)))
df1 = len(beta_hat2) - len_beta_hat3
df2 = len(y) - len(beta_hat2)
p = 1 - f.ppf(F, df1, df2)
print(F)
print(p)
x_f = np.arange(0.01, 4, 0.01)
y_f = f.pdf(x_f, df1, df2)
plt.figure()
plt.plot(x_f, y_f)
plt.plot(F, f.pdf(F, df1, df2), 'ro')
plt.xlabel('F')
plt.ylabel('Probability density')
plt.show()
# Chunk 8
def square(x):
return(x**2)
x = np.arange(0, 6.1, 0.1)
y_true = square(x)
np.random.seed(7)
y_noise = y_true + np.random.normal(scale=5, size=len(y_true))
plt.figure()
plt.plot(x, y_noise, 'bo')
plt.plot(x, y_true, 'r-')
plt.legend(['noise', 'true'])
plt.show()
# Chunk 10
# Exercise 2.2
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(x.reshape(-1, 1), y_noise)
beta0 = regressor.intercept_
beta1 = regressor.coef_[0]
y_linear_hat = beta0 + beta1 * x
plt.plot(x, y_linear_hat, 'g-')
plt.legend(['noise', 'true', 'linear fit'])
plt.show()
# Chunk 12
# Exercise 2.2.ii
from sklearn.preprocessing import PolynomialFeatures
quadratic = PolynomialFeatures(degree=2)
X_quadratic = quadratic.fit_transform(x.reshape(-1, 1))
regressor = LinearRegression()
regressor.fit(X_quadratic, y_noise)
y_quadratic_hat = regressor.intercept_ + regressor.coef_[1] * x + regressor.coef_[2] * x**2
plt.plot(x, y_quadratic_hat, 'k-')
plt.legend(['noise', 'true', 'linear fit', 'quadratic fit'])
plt.show()
# Exercise 2.2.ii
fifth = PolynomialFeatures(degree=5)
X_fifth = fifth.fit_transform(x.reshape(-1, 1))
regressor = LinearRegression()
regressor.fit(X_fifth, y_noise)
y_fifth_hat = regressor.intercept_ + regressor.coef_[1] * x + regressor.coef_[2] * x**2 + regressor.coef_[3] * x**3 + regressor.coef_[4] * x**4 + regressor.coef_[5] * x**5
plt.plot(x, y_fifth_hat, 'y-')
plt.legend(['noise', 'true', 'linear fit', 'quadratic fit', 'fifth fit'])
plt.show()
# Chunk 13
def simulate_fit_and_plot(x, n_samples, degrees, sigma, x_highlight):
predictions = list()
for degree in degrees:
predictions.append(list())
np.random.seed(7)
plt.figure()
colour_strings = ['g-', 'k-', 'k-', 'k-', 'y-']
for _ in range(n_samples):
y_true = x**2
y_sample = y_true + np.random.normal(scale=sigma, size=len(x))
for degree_index, degree in enumerate(degrees):
poly = PolynomialFeatures(degree=degree)
X = poly.fit_transform(x.reshape(-1, 1))
regressor = LinearRegression(fit_intercept=False)
regressor.fit(X, y_sample)
y_hat = np.zeros(len(x))
for number in range(degree+1):
y_hat += regressor.coef_[number] * x**number
predictions[degree_index].append(y_hat[x == x_highlight][0])
plt.plot(x, y_hat, colour_strings[degree-1])
plt.plot(x_highlight, x_highlight**2, 'ro')
plt.legend(['Degree ' + str(degrees[0]), 'Degree ' + str(degrees[1])])
plt.show()
return predictions
predictions_1_and_2 = simulate_fit_and_plot(x, 100, [1, 2], 5, 3)
predictions_2_and_5 = simulate_fit_and_plot(x, 100, [2, 5], 5, 3)
bias_1 = np.mean(predictions_1_and_2[0]) - y_true[x == 3]
bias_2 = np.mean(predictions_1_and_2[1]) - y_true[x == 3]
bias_5 = np.mean(predictions_2_and_5[1]) - y_true[x == 3]
variance_1 = np.var(predictions_1_and_2[0])
variance_2 = np.var(predictions_1_and_2[1])
variance_5 = np.var(predictions_2_and_5[1])
bias_1
bias_2
bias_5
variance_1
variance_2
variance_5
epsilon = np.random.normal(scale=5, size=100)
MSE_1 = bias_1**2 + variance_1 + np.var(epsilon)
MSE_2 = bias_2**2 + variance_2 + np.var(epsilon)
MSE_5 = bias_5**2 + variance_5 + np.var(epsilon)
MSE_1
MSE_2
MSE_5
x.reshape(-1, 1)
x
x.reshape(-1, 1)
x
quadratic = PolynomialFeatures(degree=2)
X_quadratic = quadratic.fit_transform(x.reshape(-1, 1))
X_quadratic
x
x.reshape(-1, 1)
X_quadratic
6.0**2
5.9**2
