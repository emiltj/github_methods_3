X == Z @ W.T
W = eigen_vect
X = data_248_sc
Z = X @ W
X == Z @ W.T
print(np.sum(np.isclose(X, Z)) == X.size)
Z = X @ W
X == Z @ W.T
X == Z@W.T
print(np.sum(np.isclose(X_std, X_std_pca)) == X_std.size)
Z = X @ W
X == Z@W.T
print(np.sum(np.isclose(X, Z@W.T)) == X.size)
def equalize_targets(data, y):
np.random.seed(7)
targets = np.unique(y)
counts = list()
indices = list()
for target in targets:
counts.append(np.sum(y == target))
indices.append(np.where(y == target)[0])
min_count = np.min(counts)
first_choice = np.random.choice(indices[0], size=min_count, replace=False)
second_choice = np.random.choice(indices[1], size=min_count, replace=False)
third_choice = np.random.choice(indices[2], size=min_count, replace=False)
fourth_choice = np.random.choice(indices[3], size=min_count, replace=False)
new_indices = np.concatenate((first_choice, second_choice,
third_choice, fourth_choice))
new_y = y[new_indices]
new_data = data[new_indices, :, :]
return new_data, new_y
quit
# Import everything needed
import numpy as np
import matplotlib.pyplot as plt
from os.path import join
from scipy import linalg
# Load in data
data = np.load(os.path.join("..", "week_08", "data", "megmag_data.npy"))
y = np.load(os.path.join("..", "week_08", "data", "pas_vector.npy"))
# Equalize data
data, y = equalize_targets(data, y)
times = np.arange(-200, 804, 4)
index_248 = int(list(np.where(times == 248))[0])
# Which shape?
data.shape # 3rd dim seems to be time
# Subset to only get column with timepoint = 248 ms.
data_248 = data[:,:,index_248]
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
data_248_sc = sc.fit_transform(data_248)
cov = np.cov(data_248_sc.T) # Why transpose? ->
data_248_sc.shape # Shape is sensors on 2nd dim (columns)
?np.cov # We want it as first dim -> read documentation
import matplotlib.pyplot as plt
plt.figure()
plt.imshow(cov)
plt.show()
# That they aren't independent.
# Especially between sensors with similar indices (close to vertical line)
np.linalg.matrix_rank(cov)
#97
eigen_val, eigen_vect = np.linalg.eig(cov)
eigen_val = np.real(eigen_val)
eigen_vect = np.real(eigen_vect)
pass
eigen_val = np.abs(eigen_val)
sorted_indices = np.flip(np.argsort(eigen_val))
sorted_indices
eigen_val = eigen_val[sorted_indices]
eigen_vect = eigen_vect[sorted_indices]
plt.figure()
plt.plot(np.log(eigen_val), "o")
plt.show()
# Some eigenvalues (when log-transformed) have clearly different values. We're talking about the last 5
W = eigen_vect
# Called it X as Lau wants
X = data_248_sc
# Define Z
Z = X @ W
# See that X = Z@W.T
X == Z@W.T
print(np.sum(np.isclose(X, Z@W.T)) == X.size)
eigen_vect = eigen_vect[:, sorted_indices]
W = eigen_vect
eigen_vect = eigen_vect[:, sorted_indices]
W = eigen_vect
X = data_248_sc
# Define Z
Z = X @ W
# See that X = Z@W.T
X == Z@W.T
print(np.sum(np.isclose(X, Z@W.T)) == X.size)
Z = X @ W
# See that X = Z @ W.T
X == Z@W.T
# or another way
print(np.sum(np.isclose(X, Z @ W.T)) == X.size)
times = np.arange(-200, 804, 4)
index_248 = int(list(np.where(times == 248))[0])
# Which shape?
data.shape # 3rd dim seems to be time
# Subset to only get column with timepoint = 248 ms.
data_248 = data[:,:,index_248]
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
data_248_sc = sc.fit_transform(data_248)
cov = np.cov(data_248_sc.T) # Why transpose? ->
data_248_sc.shape # Shape is sensors on 2nd dim (columns)
?np.cov # We want it as first dim -> read documentation
import matplotlib.pyplot as plt
plt.figure()
plt.imshow(cov)
plt.show()
np.linalg.matrix_rank(cov)
rank = np.linalg.matrix_rank(cov)
plt.figure()
plt.plot(np.log(eigen_val), "o")
plt.show()
Z = X @ W
X_std_pca = Z @ W.T
print(np.sum(np.isclose(X, X_std_pca)) == X.size)
X = data_248_sc
Z = X @ W
X_std_pca = Z @ W.T
print(np.sum(np.isclose(X, X_std_pca)) == X.size)
def equalize_targets(data, y):
np.random.seed(7)
targets = np.unique(y)
counts = list()
indices = list()
for target in targets:
counts.append(np.sum(y == target))
indices.append(np.where(y == target)[0])
min_count = np.min(counts)
first_choice = np.random.choice(indices[0], size=min_count, replace=False)
second_choice = np.random.choice(indices[1], size=min_count, replace=False)
third_choice = np.random.choice(indices[2], size=min_count, replace=False)
fourth_choice = np.random.choice(indices[3], size=min_count, replace=False)
new_indices = np.concatenate((first_choice, second_choice,
third_choice, fourth_choice))
new_y = y[new_indices]
new_data = data[new_indices, :, :]
return new_data, new_y
## probably reduce to a single time (or a few time points)
import numpy as np
import matplotlib.pyplot as plt
from os.path import join
from scipy import linalg
# i.
path = '/home/lau/Skrivebord/class_subject'
data = np.load(join(path, 'megmag_data.npy'))
y = np.load(join(path, 'pas_vector.npy'))
# ii.
data, y = equalize_targets(data, y)
# iii.
times = np.arange(-200, 804, 4)
max_time_index = np.where(times == 248)[0][0]
X = data[:, :, 112] ## max time point
# iv.
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_std = sc.fit_transform(X)
# v.
cov = np.cov(X_std.T)
plt.figure()
plt.imshow(cov)
plt.show()
# vii.
rank = np.linalg.matrix_rank(cov)
# viii.
eigenvalues, eigenvectors = np.linalg.eig(cov)
eigenvalues = np.real(eigenvalues)
eigenvectors = np.real(eigenvectors)
# i.
eigenvalues = np.abs(eigenvalues)
# ii.
sorted_indices = np.flip(np.argsort(eigenvalues))
# iii.
eigenvalues = eigenvalues[sorted_indices]
eigenvectors = eigenvectors[:, sorted_indices]
# iv.
plt.figure()
plt.plot(np.log(eigenvalues), 'bo')
plt.show()
# v.
W = eigenvectors
# vi.
Z = X_std @ W
X_std_pca = Z @ W.T
print(np.sum(np.isclose(X_std, X_std_pca)) == X_std.size)
# vii.
Z_cov = np.cov(Z.T)
plt.figure()
plt.imshow(Z_cov)
plt.show()
plt.figure()
plt.plot(np.log(eigenvalues), 'bo')
plt.show()
# v.
W = eigenvectors
# vi.
Z = X_std @ W
X_std_pca = Z @ W.T
Z = X_std @ W
reticulate::repl_python()
def equalize_targets(data, y):
np.random.seed(7)
targets = np.unique(y)
counts = list()
indices = list()
for target in targets:
counts.append(np.sum(y == target))
indices.append(np.where(y == target)[0])
min_count = np.min(counts)
first_choice = np.random.choice(indices[0], size=min_count, replace=False)
second_choice = np.random.choice(indices[1], size=min_count, replace=False)
third_choice = np.random.choice(indices[2], size=min_count, replace=False)
fourth_choice = np.random.choice(indices[3], size=min_count, replace=False)
new_indices = np.concatenate((first_choice, second_choice,
third_choice, fourth_choice))
new_y = y[new_indices]
new_data = data[new_indices, :, :]
return new_data, new_y
# Import everything needed
import numpy as np
import matplotlib.pyplot as plt
from os.path import join
from scipy import linalg
pass
# Load in data
data = np.load(os.path.join("..", "week_08", "data", "megmag_data.npy"))
y = np.load(os.path.join("..", "week_08", "data", "pas_vector.npy"))
# Equalize data
data, y = equalize_targets(data, y)
times = np.arange(-200, 804, 4)
index_248 = int(list(np.where(times == 248))[0])
# Which shape?
data.shape # 3rd dim seems to be time
# Subset to only get column with timepoint = 248 ms.
X = data[:,:,index_248]
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_std = sc.fit_transform(X)
cov = np.cov(X_std.T) # Why transpose? ->
X_std.shape # Shape is sensors on 2nd dim (columns)
?np.cov # We want it as first dim -> read documentation
import matplotlib.pyplot as plt
plt.figure()
plt.imshow(cov)
plt.show()
# That they aren't independent.
# Especially between sensors with similar indices (close to vertical line)
rank = np.linalg.matrix_rank(cov)
rank
#97
eigen_val, eigen_vect = np.linalg.eig(cov)
eigen_val = np.real(eigen_val)
eigen_vect = np.real(eigen_vect)
pass
eigen_val = np.abs(eigen_val)
sorted_indices = np.flip(np.argsort(eigen_val))
sorted_indices
eigen_val = eigen_val[sorted_indices]
eigen_vect = eigen_vect[:, sorted_indices]
plt.figure()
plt.plot(np.log(eigen_val), "o")
plt.show()
# Some eigenvalues (when log-transformed) have clearly different values. We're talking about the last 5
W = eigen_vect
# Remember to have: eigen_vect = eigen_vect[:, sorted_indices]
# Called it X as Lau wants
# Define Z
Z = X_std @ W
# See that X = Z @ W.T
X_std == Z @ W.T
# or another way
print(np.sum(np.isclose(X_std, Z @ W.T)) == X_std.size)
####### Not working
#### Lau
X_std.T == Z @ W.T
X_std.T == Z @ W.T
X_std == Z @ W.T
# or another way
print(np.sum(np.isclose(X_std, Z @ W.T)) == X_std.size)
print(np.sum(np.isclose(X_std, Z @ W.T)) == X_std.size)
np.cov(Z)
cov_z = np.cov(Z)
plt.figure()
plt.imshow(cov_z)
plt.show()
cov_z = np.cov(Z)
plt.figure()
plt.imshow(cov_z)
plt.show()
cov_z = np.cov(Z.T)
plt.figure()
plt.imshow(cov_z)
plt.show()
cov_z.shape
from sklearn.linear_model import LogisticRegression
Z
Z.shape
Z.shape
print(i)
for i in np.arange(1, 102):
print(i)
s```
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
logr = LogisticRegression(penalty='none', solver='newton-cg')
# Inspect Z
Z.shape # 396 Trials, 102 Principal Components (features)
# We want to fit a model using only first PC, then first two PCs, etc.
ks = np.arange(1, 103)
accs = []
for k in ks:
Z_only_k = Z[:,0:k+1]
logr.fit(Z_only_k, y)
predictions = logr.predict(Z_only_k)
acc = accuracy_score(predictions, y)
print(x)
plt.figure()
plt.plot(ks, accs, "g-")
plt.show()
ks
accs
accs = []
for k in ks:
Z_only_k = Z[:,0:k+1]
logr.fit(Z_only_k, y)
predictions = logr.predict(Z_only_k)
acc = accuracy_score(predictions, y)
accs.append(acc)
ks
plt.figure()
plt.plot(ks, accs, "g-")
plt.show()
reticulate::repl_python()
def equalize_targets(data, y):
np.random.seed(7)
targets = np.unique(y)
counts = list()
indices = list()
for target in targets:
counts.append(np.sum(y == target))
indices.append(np.where(y == target)[0])
min_count = np.min(counts)
first_choice = np.random.choice(indices[0], size=min_count, replace=False)
second_choice = np.random.choice(indices[1], size=min_count, replace=False)
third_choice = np.random.choice(indices[2], size=min_count, replace=False)
fourth_choice = np.random.choice(indices[3], size=min_count, replace=False)
new_indices = np.concatenate((first_choice, second_choice,
third_choice, fourth_choice))
new_y = y[new_indices]
new_data = data[new_indices, :, :]
return new_data, new_y
# Import everything needed
import numpy as np
import matplotlib.pyplot as plt
from os.path import join
from scipy import linalg
pass
# Load in data
data = np.load(os.path.join("..", "week_08", "data", "megmag_data.npy"))
y = np.load(os.path.join("..", "week_08", "data", "pas_vector.npy"))
# Equalize data
data, y = equalize_targets(data, y)
times = np.arange(-200, 804, 4)
index_248 = int(list(np.where(times == 248))[0])
# Which shape?
data.shape # 3rd dim seems to be time
# Subset to only get column with timepoint = 248 ms.
X = data[:,:,index_248]
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_std = sc.fit_transform(X)
cov = np.cov(X_std.T) # Why transpose? ->
X_std.shape # Shape is sensors on 2nd dim (columns)
?np.cov # We want it as first dim -> read documentation
import matplotlib.pyplot as plt
plt.figure()
plt.imshow(cov)
plt.show()
# That they aren't independent.
# Especially between sensors with similar indices (close to vertical line)
rank = np.linalg.matrix_rank(cov)
rank
#97
eigen_val, eigen_vect = np.linalg.eig(cov)
eigen_val = np.real(eigen_val)
eigen_vect = np.real(eigen_vect)
eigen_val = np.abs(eigen_val)
sorted_indices = np.flip(np.argsort(eigen_val))
sorted_indices
eigen_val = eigen_val[sorted_indices]
eigen_vect = eigen_vect[:, sorted_indices]
plt.figure()
plt.plot(np.log(eigen_val), "o")
plt.show()
# Some eigenvalues (when log-transformed) have clearly different values. We're talking about the last 5
W = eigen_vect
# Remember to have: eigen_vect = eigen_vect[:, sorted_indices]
# Called it X as Lau wants
# Define Z (principal components)
Z = X_std @ W
# See that X = Z @ W.T
X_std == Z @ W.T
# or another way
print(np.sum(np.isclose(X_std, Z @ W.T)) == X_std.size)
#### Lau
# Why isn't my method working?
# What are you doing?
# What have we done so far, in general?
cov_z = np.cov(Z.T)
plt.figure()
plt.imshow(cov_z)
plt.show()
#### Lau
# What has happened here?
# I expected no covariance between components, but I expected perfect covariance between the same principal components? Why do we not see that here?
# Why transpose it here? Don't we have the same on both dimensions?
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
logr = LogisticRegression(penalty='none', solver='newton-cg')
# Inspect Z
Z.shape # 396 Trials, 102 Principal Components (features)
# We want to fit a model using only first PC, then first two PCs, etc.
ks = np.arange(1, 103)
accs = []
for k in ks:
Z_only_k = Z[:,0:k+1]
logr.fit(Z_only_k, y)
predictions = logr.predict(Z_only_k)
acc = accuracy_score(predictions, y)
accs.append(acc)
# Plot accuracy over Ks
plt.figure()
plt.plot(ks, accs, "g-")
plt.show()
# The general trend is = higher K, better performance.
# With K = 102, we have all the data
# The last 5 components explain very little variance.
# The last 5 eigenvalues (when log-transformed), where very low.
#### Lau
# Would you explain the relationship between eigenvalues and PC
from sklearn.model_selection import cross_val_score, StratifiedKFold
cv = StratifiedKFold()
# We want to fit a model using only first PC, then first two PCs, etc.
ks = np.arange(1, 103)
accs_cv = []
for k in ks:
Z_only_k = Z[:,0:k+1]
acc_cv = np.mean(cross_val_score(logr, Z_only_k, y, cv=cv))
accs_cv.append(acc_cv)
print("done")
plt.figure()
plt.plot(ks, accs_cv, "g-")
plt.show()
max_index = np.argmax(accs_cv)+1
max_index
pct_points_incr = accs_cv[max_index] - accs_cv[-1]
pct_points_incr # 6.84 percentage points.
((accs_cv[max_index] - accs_cv[-1]) / accs_cv[-1]) * 100 # 20.1 percent increase
from sklearn.decomposition import PCA
# initialize PCA object. Number of components should be the one with maximum performance when we did the cv for one of the timepoints.
pca = PCA(n_components=max_index)
logr = LogisticRegression(penalty='none', solver='newton-cg', max_iter=250)
cv = StratifiedKFold()
acc_time_pca = []
acc_time_full = []
for i in np.arange(0, 5):#251):
X = data[:,:,i]
X_std = sc.fit_transform(X)
X_std_pca = pca.fit_transform(X_std)
acc_time_full.append(np.mean(cross_val_score(logr, X_std, y, cv=cv)))
acc_time_pca.append(np.mean(cross_val_score(logr, X_std_pca, y, cv=cv)))
print("done")
plt.figure()
plt.plot(times[0:5], acc_time_pca, "r-")
plt.plot(times[0:5], acc_time_full, "g-")
plt.legend(['With PCA', 'Without PCA'])
plt.axhline(0.25, color='k')
plt.axvline(color='k')
plt.show()
pca = PCA(n_components=max_index)
logr = LogisticRegression(penalty='none', solver='newton-cg', max_iter=250)
cv = StratifiedKFold()
acc_time_pca = []
acc_time_full = []
for i in np.arange(0, 251):
X = data[:,:,i]
X_std = sc.fit_transform(X)
X_std_pca = pca.fit_transform(X_std)
acc_time_full.append(np.mean(cross_val_score(logr, X_std, y, cv=cv)))
acc_time_pca.append(np.mean(cross_val_score(logr, X_std_pca, y, cv=cv)))
print("done")
plt.figure()
plt.plot(times, acc_time_pca, "r-")
plt.plot(times, acc_time_full, "g-")
plt.legend(['With PCA', 'Without PCA'])
plt.axhline(0.25, color='k')
plt.axvline(color='k')
plt.show()
plt.figure()
plt.plot(times, acc_time_pca, "r-")
plt.plot(times, acc_time_full, "g-")
plt.legend(['With PCA', 'Without PCA'])
plt.axhline(0.25, color='k')
plt.axvline(color='k')
plt.show()
